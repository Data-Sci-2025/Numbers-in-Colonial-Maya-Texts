---
title: "Numbers in Colonial Maya Texts"
author: "Alex LaPrevotte"
date: "12 December 2025"
format: gfm
---

```{r setup, include=FALSE}
library(tidyverse)
library(pdftools)
library(dplyr)
library(stringr)
library(purrr)
```

**I have opted to add my new code, as I go, to my existing quarto document.**

My goal for processing these data is to get the text into a format I can work with. The data, available as part of "Los títulos de ebtún : transcripción, traducción y análisis histórico," the PhD dissertation of Dr. Julien Machault, which can be found [here](https://tesiunamdocumentos.dgb.unam.mx/ptd2025/abr_jun/0869324/Index.html), are currently in a PDF, broken into chunks of five lines:

* a transcription of the original colonial Maya text (in original orthography)
* a transcription of the text with modern orthography
* a morphological breakdown
* a morphological gloss
* a Spanish translation.
  
I am working with a 14-page excerpt from this PDF (pages 58-71), selected because it begins and ends with complete 5-line chunks of text, but I am interested in trying to expand to a larger sample in the future and I am attempting to account for situations that do not arise in the current data within the code, so it could be applied to a larger data set. I anticipate my working data, pre-analysis, will take the form of a data frame with two columns (morphological breakdown, morphological gloss) and many rows, one per word. I plan to eliminate the original orthography, modern orthography, and Spanish translation because they contain special characters, do not correspond well with the other lines in number of words, and the only way to correlate the Spanish translations with the Maya would be manually, due to differences in word order and agglutination. 

## Reading in the data

```{r data-read-in}
# read in the text with each page as a row
ebtun_text <-pdf_text("data/ebtun_sample.pdf")

# lines to rows
ebtun_text <- strsplit(ebtun_text, "\n")

# combine all pages into one vector
ebtun_text <- unlist(ebtun_text)

# currently 1 column, 543 rows
```

## Combining morphological breakdowns into single units

```{r fixing-split-words}
# eliminating multiple spaces and spaces before and after hyphens
# this should keep morphological breakdowns and glosses together
ebtun_text <- str_squish(gsub("-[ /t]+","-",gsub("[ /t]+-","-",ebtun_text)))

# eliminating null characters
ebtun_text <- str_replace_all(ebtun_text, "\\bø\\b", "")

# convert to dataframe
ebtun_data <- data.frame(line = ebtun_text)
```

## Cleaning up the rows

```{r row-cleaning}
# eliminate rows that only include numbers or spaces
ebtun_data <- subset(ebtun_data, !grepl("^\\s*$", line) & !grepl("^\\s*\\d+\\s*$", line))

# reset row numbers
rownames(ebtun_data) <- NULL

# eliminate a 4-row footnote
ebtun_data <- ebtun_data[-c(195:198), ]

# make it a dataframe again
ebtun_data <- data.frame(line = ebtun_data, stringsAsFactors = FALSE)

# currently 1 column, 340 rows
```

## Breaking the data into chunks

```{r data-to-chunks}
# my goal here is to break the data back down into the 5-line chunks that were the basis of the PDF formatting

# every 5 lines to a group
ebtun_grouped <- ebtun_data |>
  mutate(group = ceiling(row_number() / 5))

# each group to a dataframe
ebtun_chunks <- split(ebtun_grouped, ebtun_grouped$group)

# currently 68 data frames, each two columns (text and group number) and five rows
```

## Removing rows not used in analysis

```{r whittling-rows}
# eliminating rows 1, 2, and 5 from each chunk
ebtun_chunks <- lapply(ebtun_chunks, function(df) {
  df[3:4, , drop = FALSE]
})
```

## Split chunks by word

```{r split-by-word}
# function: split chunk into words
  # split each line by spaces
  # find max words in the chunk and add NAs to standardize columns
  # combine into data frame
split_chunk <- function(df_chunk) {
  split_words <- str_split(df_chunk$line, "\\s+", simplify = FALSE)
  max_words <- max(lengths(split_words))
  split_words_padded <- lapply(split_words, function(x) {
    length(x) <- max_words
    x
  })
  df_words <- as.data.frame(do.call(rbind, split_words_padded), stringsAsFactors = FALSE)
  return(df_words)
}

# use function on chunks
ebtun_chunks <- lapply(ebtun_chunks, split_chunk)
```

## NA wrangling

```{r NA-wrangling}
# which chunks have NAs?
na_counts <- sapply(ebtun_chunks, function(df) sum(is.na(df)))
na_counts

# eliminate chunks with NAs
# noting that eliminating chunk 51 does get rid of one of my number data points; óox means three
ebtun_chunks_nona <- ebtun_chunks[!sapply(ebtun_chunks, function(df) any(is.na(df)))]

# check to make sure number of chunks has decreased
length(ebtun_chunks_nona)
```

## Chunk concatenation

```{r chunk-concatenation, message=FALSE}
# number of rows
num_rows <- nrow(ebtun_chunks_nona[[1]])

# concatenate rows by chunk
ebtun_combined <- map_dfr(1:num_rows, function(i) {
  rows <- lapply(ebtun_chunks_nona, function(df) df[i, , drop = FALSE])
  suppressMessages(
    bind_cols(rows, .name_repair = "minimal")
  )
})

#drop the column names
colnames(ebtun_combined) <- NULL
```

## Data transposition

```{r transpose-data}
# turn those rows into columns
ebtun_long <- t(ebtun_combined)

# convert back to data frame
ebtun <- as.data.frame(ebtun_long)

# currently 2 columns (morphological breakdown and morphological gloss), 349 rows
```

## Final data cleaning

```{r final-data-cleaning}
# removing leading and trailing spaces
ebtun$V1 <- str_trim(ebtun$V1)
ebtun$V2 <- str_trim(ebtun$V2)

# remove rows containing cells that are just ellipses (either the ellipses character or three consecutive periods)
ebtun <- ebtun |>
  filter(if_all(everything(), ~ !str_trim(.) %in% c("...", "…")))

#naming the columns because I'm about to add more and it seems responsible
ebtun <- ebtun |>
  rename(
    breakdown = V1,
    gloss = V2
  )

# final form of "found" data
str(ebtun)
```

## Analysis

```{r analysis, message=FALSE}
# creating column: is this a numeral?
ebtun <- ebtun |>
  mutate(
    numeral = str_detect(str_trim(breakdown), "^[0-9]+$")
  )

# reading in lists of Spanish and Maya numbers (textual)
esp_numbers <- read_csv("data/esp_numbers.csv", col_names = FALSE) |> pull(X1)
maya_numbers <- read_csv("data/maya_numbers.csv", col_names = FALSE) |> pull(X1)

# fix encoding for accents
esp_numbers <- iconv(esp_numbers, from = "latin1", to = "UTF-8")
maya_numbers <- iconv(maya_numbers, from = "latin1", to = "UTF-8")

# does this cell contain a spanish or maya number?
ebtun <- ebtun |>
  mutate(
    spanish_number = str_detect(breakdown, paste(esp_numbers, collapse = "|")),
    maya_number = str_detect(breakdown, paste(maya_numbers, collapse = "|"))
  )

```

```{r session-info}
sessionInfo()
```